<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <!-- Primary Meta Tags -->
  <meta name="title" content="Look Where It Matters: Training-Free Ultra-HR Remote Sensing VQA - Yunqi Zhou, Chengjie Jiang, Chun Yuan, Jing Li">
  <meta name="description" content="A training-free approach for ultra-high resolution remote sensing visual question answering using adaptive zoom search to efficiently process large-scale satellite imagery.">
  <meta name="keywords" content="remote sensing, visual question answering, ultra-high resolution, training-free, adaptive zoom search, computer vision, satellite imagery, multimodal AI">
  <meta name="author" content="Yunqi Zhou, Chengjie Jiang, Chun Yuan, Jing Li">
  <meta name="robots" content="index, follow">
  <meta name="language" content="English">
  
  <!-- Open Graph / Facebook -->
  <meta property="og:type" content="article">
  <meta property="og:site_name" content="Tsinghua University & CUFE & ECNU">
  <meta property="og:title" content="Look Where It Matters: Training-Free Ultra-HR Remote Sensing VQA">
  <meta property="og:description" content="A training-free approach for ultra-high resolution remote sensing visual question answering using adaptive zoom search to efficiently process large-scale satellite imagery.">
  <!-- <meta property="og:url" content="https://YOUR_DOMAIN.com/look-where-it-matters">
  <meta property="og:image" content="https://YOUR_DOMAIN.com/static/images/social_preview.png">
  <meta property="og:image:width" content="1200">
  <meta property="og:image:height" content="630"> -->
  <meta property="og:image:alt" content="Look Where It Matters - Research Preview">
  <meta property="article:author" content="Yunqi Zhou">
  <meta property="article:author" content="Chengjie Jiang">
  <meta property="article:section" content="Research">
  <meta property="article:tag" content="Remote Sensing">
  <meta property="article:tag" content="Ultra High Resolution Remote Sensing Visual Question Answering">

  <!-- Academic/Research Specific -->
  <meta name="citation_title" content="Look Where It Matters: Training-Free Ultra-HR Remote Sensing VQA via Adaptive Zoom Search">
  <meta name="citation_author" content="Zhou, Yunqi">
  <meta name="citation_author" content="Jiang, Chengjie">
  <meta name="citation_author" content="Yuan, Chun">
  <meta name="citation_author" content="Li, Jing">
  <!-- <meta name="citation_publication_date" content="2024">
  <meta name="citation_conference_title" content="CONFERENCE_NAME">
  <meta name="citation_pdf_url" content="https://YOUR_DOMAIN.com/static/pdfs/paper.pdf"> -->
  
  <!-- Additional SEO -->
  <meta name="theme-color" content="#2563eb">
  <meta name="msapplication-TileColor" content="#2563eb">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="default">
  
  <!-- Preconnect for performance -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link rel="preconnect" href="https://ajax.googleapis.com">
  <link rel="preconnect" href="https://documentcloud.adobe.com">
  <link rel="preconnect" href="https://cdn.jsdelivr.net">

  <title>Look Where It Matters: Training-Free Ultra-HR Remote Sensing VQA | Academic Research</title>
  
  <!-- Favicon and App Icons -->
  <link rel="icon" type="image/png" href="static/images/logo.png">
  <link rel="apple-touch-icon" href="static/images/logo.png">
  
  <!-- Critical CSS - Load synchronously -->
  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/index.css">
  
  <!-- Non-critical CSS - Load asynchronously -->
  <link rel="preload" href="static/css/bulma-carousel.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="static/css/bulma-slider.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="static/css/fontawesome.all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  <link rel="preload" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
  
  <!-- Fallback for browsers that don't support preload -->
  <noscript>
    <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  </noscript>
  
  <!-- Fonts - Optimized loading -->
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700;800&display=swap" rel="stylesheet">
  
  <!-- Defer non-critical JavaScript -->
  <script defer src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script defer src="static/js/bulma-carousel.min.js"></script>
  <script defer src="static/js/bulma-slider.min.js"></script>
  <script defer src="static/js/index.js"></script>
  
  <!-- Structured Data for Academic Papers -->
  <script type="application/ld+json">
  {
    "@context": "https://schema.org",
    "@type": "ScholarlyArticle",
    "headline": "Look Where It Matters: Training-Free Ultra-HR Remote Sensing VQA via Adaptive Zoom Search",
    "author": [
      {
        "@type": "Person",
        "name": "Yunqi Zhou",
        "affiliation": {
          "@type": "Organization",
          "name": "Central University of Finance and Economics"
        }
      },
      {
        "@type": "Person",
        "name": "Chengjie Jiang",
        "affiliation": {
          "@type": "Organization",
          "name": "Tsinghua University"
        }
      },
      {
        "@type": "Person",
        "name": "Chun Yuan",
        "affiliation": {
          "@type": "Organization",
          "name": "Tsinghua University"
        }
      },
      {
        "@type": "Person",
        "name": "Jing Li",
        "affiliation": {
          "@type": "Organization",
          "name": "East China Normal University"
        }
      }
    ],
    "keywords": ["remote sensing", "visual question answering", "ultra-high resolution", "training-free", "adaptive zoom search", "computer vision", "satellite imagery"],
    "abstract": "With advances in satellite constellations, sensor technologies, and imaging pipelines, ultra-high-resolution (Ultra-HR) remote sensing imagery is becoming increasingly widespread. However, current remote sensing foundation models are ill-suited to such inputs: full-image encoding exhausts token and memory budgets, while resize-based preprocessing loses fine-grained and answer-critical details. In this context, guiding the model look where it matters before prediction becomes crucial. Therefore, we present ZoomSearch, a training-free, plug-and-play pipeline that decouples 'where to look' from 'how to answer' for Ultra-HR Remote Sensing Visual Question Answering (RS-VQA). ZoomSearch combines Adaptive Multi-Branch Zoom Search, which performs a hierarchical search over image patches to localize query-relevant regions, with Layout-Aware Patch Reassembly, which reorganizes the selected patches into a compact, layout-faithful canvas. We conduct comprehensive experiments on Ultra-HR RS-VQA benchmarks MME-RealWorld-RS and LRS-VQA, comparing against (i) strong general foundation models, (ii) remote sensing foundation models, (iii) Ultra-HR RS-VQA methods, and (iv) plug-and-play search-based VQA methods. When integrated with LLaVA-ov, ZoomSearch attains state-of-the-art accuracy across diverse tasks, improving the LLaVA-ov baseline by 26.3% on LRS-VQA and 114.8% on MME-RealWorld-RS. Meanwhile, it achieves much higher inference efficiency, outperforming prior search-based methods by 20–44% in speed.",
    "isAccessibleForFree": true,
    "license": "https://creativecommons.org/licenses/by/4.0/",
    "mainEntity": {
      "@type": "WebPage",
      "@id": "https://kiki-zyq.github.io/ZS/"
    },
    "about": [
      {
        "@type": "Thing",
        "name": "Remote Sensing"
      },
      {
        "@type": "Thing", 
        "name": "Ultra High Resolution Visual Question Answering"
      }
    ]
  }
  </script>
</head>
<body>


  <!-- Scroll to Top Button -->
  <button class="scroll-to-top" onclick="scrollToTop()" title="Scroll to top" aria-label="Scroll to top">
    <i class="fas fa-chevron-up"></i>
  </button>

  <main id="main-content">
  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-widescreen">
        <div class="columns is-centered">
          <div class="column has-text-centered">
              <div style="text-align: center; margin-bottom: 1rem; font-size: 2rem; line-height: 1; font-family: 'Helvetica Neue', Helvetica, Arial, sans-serif; font-weight: 700;">
                <div style="display: inline-flex; align-items: center;">
                  <img src="static/images/logo.png" alt="Logo" style="height: 50px; margin-right: 12px;">
                  <span style="margin: 0; line-height: 1;">Look Where It Matters:</span>
                </div>
                <div style="margin: 0; line-height: 1;">
                  Training-Free Ultra-HR Remote Sensing VQA via Adaptive Zoom Search
                </div>
              </div>



            <div class="is-size-4 publication-authors">
              <span class="author-block">
                <a href="https://scholar.google.com/citations?user=h3QTqt8AAAAJ&hl" target="_blank">Yunqi Zhou</a><sup>1*</sup>,</span>
                <span class="author-block">
                  <a href="https://scholar.google.com/citations?user=NGizEIYAAAAJ&hl" target="_blank">Chengjie Jiang</a><sup>2*</sup>,</span>
                  <span class="author-block">
                    <a href="https://scholar.google.com/citations?user=fYdxi2sAAAAJ" target="_blank">Chun Yuan</a><sup>2</sup>,</span>
                  <span class="author-block">
                    <a href="https://scholar.google.com/citations?hl=zh-CN&user=YAG9tSMAAAAJ" target="_blank">Jing Li</a><sup>3†</sup>
                  </span>
                  </div>

                  <div class="is-size-4 publication-authors">
                    <span class="author-block"><sup>1</sup>Central University of Finance and Economics&nbsp;&nbsp;</span>
                    <span class="author-block"><sup>2</sup>Tsinghua University&nbsp;&nbsp;</span>
                    <span class="author-block"><sup>3</sup>East China Normal University</span>
                    <span class="eql-cntrb" style="display:block; margin-top:0.2rem; line-height:1.2;">
                      <small><sup>*</sup>Equal Contribution&nbsp;&nbsp;<sup>†</sup>Corresponding Author</small>
                    </span>
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                      <span class="link-block">
                        <a href="https://arxiv.org/pdf/<ARXIV PAPER ID>.pdf" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                  <span class="link-block">
                    <a href="https://github.com/YOUR_REPO_HERE" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code(coming soon)</span>
                  </a>
                </span>

            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            With advances in satellite constellations, sensor technologies, and imaging pipelines, ultra-high-resolution (Ultra-HR) remote sensing imagery is becoming increasingly widespread. However, current remote sensing foundation models are ill-suited to such inputs: full-image encoding exhausts token and memory budgets, while resize-based preprocessing loses fine-grained and answer-critical details. In this context, guiding the model <strong>look where it matters</strong> before prediction becomes crucial. Therefore, we present <strong>ZoomSearch</strong>, a training-free, plug-and-play pipeline that decouples 'where to look' from 'how to answer' for Ultra-HR Remote Sensing Visual Question Answering (RS-VQA). ZoomSearch combines <strong>Adaptive Multi-Branch Zoom Search</strong>, which performs a hierarchical search over image patches to localize query-relevant regions, with <strong>Layout-Aware Patch Reassembly</strong>, which reorganizes the selected patches into a compact, layout-faithful canvas. We conduct comprehensive experiments on Ultra-HR RS-VQA benchmarks MME-RealWorld-RS and LRS-VQA, comparing against (i) strong general foundation models, (ii) remote sensing foundation models, (iii) Ultra-HR RS-VQA methods, and (iv) plug-and-play search-based VQA methods. When integrated with LLaVA-ov, <strong>ZoomSearch</strong> attains state-of-the-art accuracy across diverse tasks, improving the LLaVA-ov baseline by <strong>26.3%</strong> on LRS-VQA and <strong>114.8%</strong> on MME-RealWorld-RS. Meanwhile, it achieves much higher inference efficiency, outperforming prior search-based methods by <strong>20–44%</strong> in speed.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->

<!-- Introduction Hero Figure -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Overview of ZoomSearch</h2>

          <figure>
            <img 
              src="static/images/pipeline.jpg" 
              alt="ZoomSearch introduction figure"
              style="max-width: 100%; border-radius: 12px; box-shadow: 0 4px 14px rgba(0,0,0,0.15);" 
            />
          </figure>

          <div class="content has-text-justified mt-4">
            <p>
              Overview of the proposed <strong>ZoomSearch</strong> pipeline for Ultra-HR RS-VQA. 
              The top-left part illustrates <strong>Adaptive Multi-Branch Zoom Search</strong>, 
              which progressively explores the image and focuses on regions that are 
              closely related to the text query. 
              The bottom part shows the <strong>scoring mechanism</strong>, where each candidate patch 
              is evaluated by a patch–text relevance score from an external scoring model 
              and a model-evidence signal from the foundation model. 
              The top-right part depicts <strong>Layout-Aware Patch Reassembly</strong>, 
              which reorganizes the selected informative patches into a spatially 
              consistent canvas that preserves their relative and global positions.
            </p>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End Introduction Hero Figure -->

<!-- Image carousel -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Case Study</h2>
          <style>
            #results-carousel {
              overflow: visible !important;
            }
            
            #results-carousel .slider {
              overflow: visible !important;
            }
            
            /* 将箭头移到图片外侧 */
            #results-carousel .slider-navigation-previous {
              left: -60px !important;
            }
            
            #results-carousel .slider-navigation-next {
              right: -60px !important;
            }
          </style>
          
          <div id="results-carousel" class="carousel results-carousel">
              <div class="item">
                <img src="static/images/qualitative1.png" alt="Adaptive zoom search visualization" loading="lazy" style="max-width: 70%; margin: 0 auto; display: block;"/>
                <h2 class="subtitle has-text-centered">
                  Qualitative comparison between our method and other search-based methods on an object counting task.
                </h2>
              </div>
              <div class="item">
                <img src="static/images/qualitative2.png" alt="VQA results comparison" loading="lazy" style="max-width: 70%; margin: 0 auto; display: block;"/>
                <h2 class="subtitle has-text-centered">
                  Qualitative comparison between our method and other search-based methods on an object color recognition task.
                </h2>
              </div>
              <div class="item">
                <img src="static/images/good_cases.png" alt="VQA results comparison" loading="lazy" style="max-width: 70%; margin: 0 auto; display: block;"/>
                <h2 class="subtitle has-text-centered">
                   More qualitative results of ZoomSearch.
                </h2>
              </div>
            </div>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End image carousel -->



<!--BibTex citation -->
  <section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <div class="bibtex-header">
      <h2 class="title">BibTeX</h2>
    </div>
    <pre id="bibtex-code"><code>@article{ZoomSearch,
  title={Look Where It Matters: Training-Free Ultra-HR Remote Sensing VQA via Adaptive Zoom Search},
  author={Zhou, Yunqi and Jiang, Chengjie and Yuan, Chun and Li, Jing},
  journal={arXiv preprint arXiv:XXXX.XXXXX},
  year={2025}
}</code></pre>
  </div>
</section>

<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the source code of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
